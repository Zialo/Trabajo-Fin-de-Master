{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77905c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec38c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ignite\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from typing import List, Callable\n",
    "import pickle\n",
    "import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import os.path\n",
    "import cv2\n",
    "import csv\n",
    "import pandas as pd\n",
    "#from google.colab import drive\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import fnmatch\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "import joblib\n",
    "from torch_lr_finder import LRFinder\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22fe0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.8.1+cu101\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53df23",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7b1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "WD = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266faa7f",
   "metadata": {},
   "source": [
    "## Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdcce9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hodei.zia/ImitAI Project\n",
      "['2000 pruebas', '2000_Pruebas', 'MuJoCo_Dataset', 'Dataset_Mujoco', 'MuJoCo_100_1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Train_dataset',\n",
       " 'Test_dataset',\n",
       " 'MinMax_scaler.save',\n",
       " 'CP',\n",
       " 'CP2',\n",
       " 'resultados.csv',\n",
       " 'model_pytorch_017_LSTMlmYAc_Reg',\n",
       " 'model_pytorch_01796_LSTMlmYAc_Reg',\n",
       " 'model_pytorch_01021_Basico',\n",
       " 'CP3']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modificamos la carpeta \n",
    "print(os.getcwd())\n",
    "try:\n",
    "    print(os.listdir(\"Pruebas ImitAI\"))\n",
    "    os.chdir(\"Pruebas ImitAI/Dataset_Mujoco/Dataset_Mujoco\")\n",
    "except:\n",
    "    print(os.listdir(\"../../Pruebas ImitAI\"))\n",
    "    os.chdir(\"../../Pruebas ImitAI/Dataset_Mujoco/Dataset_Mujoco\")\n",
    "#os.chdir(\"../Prueba\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8226ca77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nres = red_merge(\"Train_dataset/\")\\ncv2_imshow(res)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def red_merge(carpeta_imagenes):\n",
    "    \n",
    "    length = len(fnmatch.filter(os.listdir(carpeta_imagenes + \"Top/\"), '*.png'))\n",
    "    path = carpeta_imagenes + \"Top/0.png\"\n",
    "    img = cv2.imread(path,1)\n",
    "    aux = np.zeros(np.shape(img))\n",
    "    \n",
    "    for idx in range(length):\n",
    "        path = carpeta_imagenes + \"Top/\" + str(idx) + \".png\"\n",
    "\n",
    "        #blurring and smoothin\n",
    "        img=cv2.imread(path,1)\n",
    "\n",
    "        hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "        lower_red = np.array([0,50,50])\n",
    "        upper_red = np.array([10,255,255])\n",
    "\n",
    "        mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "        res = cv2.bitwise_and(img,img, mask= mask)\n",
    "\n",
    "        aux = aux + res\n",
    "    return aux\n",
    "'''\n",
    "res = red_merge(\"Train_dataset/\")\n",
    "cv2_imshow(res)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b3cde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hodei.zia/ImitAI Project/Pruebas ImitAI/Dataset_Mujoco/Dataset_Mujoco'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe602c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hodei.zia/ImitAI Project/Pruebas ImitAI/Dataset_Mujoco/Dataset_Mujoco\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128 at 0x7F656845CAD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 79.9 ms, total: 257 ms\n",
      "Wall time: 545 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(os.getcwd())\n",
    "for i in range(1000):\n",
    "    im = Image.open(\"Test_dataset/Top/\"+str(i)+\".png\")\n",
    "display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685b892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hodei.zia/ImitAI Project/Pruebas ImitAI/Dataset_Mujoco/Dataset_Mujoco\n",
      "CPU times: user 878 ms, sys: 53 ms, total: 931 ms\n",
      "Wall time: 945 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(os.getcwd())\n",
    "for i in range(1000):\n",
    "    image = cv2.imread(\"Test_dataset/Top/\"+str(i)+\".png\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dcbde",
   "metadata": {},
   "source": [
    "### Normalizamos las acciones y obtenemos el array de tama√±os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d428d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hodei.zia/ImitAI Project/Pruebas ImitAI/Dataset_Mujoco/Dataset_Mujoco\n",
      "['Train_dataset', 'Test_dataset', 'MinMax_scaler.save', 'CP', 'CP2', 'resultados.csv', 'model_pytorch_017_LSTMlmYAc_Reg', 'model_pytorch_01796_LSTMlmYAc_Reg', 'model_pytorch_01021_Basico', 'CP3']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "\n",
    "# Velocities\n",
    "with open('Train_dataset/Train_Actions.csv') as f:\n",
    "    lines = (line for line in f if not line.startswith('#'))\n",
    "    x = np.loadtxt(lines, delimiter=',', skiprows=1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "x_transformed = scaler.fit_transform(x)\n",
    "np.savetxt('Train_dataset/Train_Actions_Normalized.csv',x_transformed , delimiter=',', header=\"X1,X2,X3\")\n",
    "\n",
    "with open('Test_dataset/Test_Actions.csv') as f:\n",
    "    lines = (line for line in f if not line.startswith('#'))\n",
    "    x = np.loadtxt(lines, delimiter=',', skiprows=1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "x_transformed = scaler.fit_transform(x)\n",
    "np.savetxt('Test_dataset/Test_Actions_Normalized.csv',x_transformed , delimiter=',', header=\"X1,X2,X3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20efac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_filename = \"MinMax_scaler.save\"\n",
    "scaler_to_save = MinMaxScaler(feature_range=(-1, 1))\n",
    "joblib.dump(scaler_to_save, scaler_filename) \n",
    "scaler_loaded = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3586df82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt_list = [\"Train_dataset\", \"Test_dataset\"]\\nty_list = [\"Top\", \"Corner\", \"Corner2\", \"Corner3\", \"Gripper\", \"BehindGripper\"]\\ncount = 0\\nfor t in t_list:\\n    for ty in ty_list:\\n        f = str(t + \"/\" + ty)\\n        print(f)\\n        for file in os.listdir(f):\\n            f_img = f+\"/\"+file\\n            img = Image.open(f_img)\\n            img = img.resize((128,128))\\n            img.save(f_img)\\n            count += 1\\n            if count % 500 == 0:\\n                print(count)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t_list = [\"Train_dataset\", \"Test_dataset\"]\n",
    "ty_list = [\"Top\", \"Corner\", \"Corner2\", \"Corner3\", \"Gripper\", \"BehindGripper\"]\n",
    "count = 0\n",
    "for t in t_list:\n",
    "    for ty in ty_list:\n",
    "        f = str(t + \"/\" + ty)\n",
    "        print(f)\n",
    "        for file in os.listdir(f):\n",
    "            f_img = f+\"/\"+file\n",
    "            img = Image.open(f_img)\n",
    "            img = img.resize((128,128))\n",
    "            img.save(f_img)\n",
    "            count += 1\n",
    "            if count % 500 == 0:\n",
    "                print(count)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a329fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clases para mas de una imagen\n",
    "\n",
    "class ObsActionGetter(object):\n",
    "    \"\"\"\n",
    "    Generic class that return the image and the action given an index\n",
    "    \"\"\"\n",
    "    def __init__(self, carpeta_imagenes, archivo_acciones):\n",
    "        # TODO: get a list of all image files\n",
    "        self.archivo_acciones = archivo_acciones\n",
    "        self.carpeta_imagenes = carpeta_imagenes\n",
    "        path, dirs, files = next(os.walk(self.carpeta_imagenes))\n",
    "        self.image_files = list(range(len(files)))\n",
    "        # TODO: get all made actions\n",
    "        self.actions = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(fnmatch.filter(os.listdir(self.carpeta_imagenes + \"Top/\"), '*.png'))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        path_1 = self.carpeta_imagenes + \"Top/\" + str(idx) + \".png\"\n",
    "        path_2 = self.carpeta_imagenes + \"Corner/\" + str(idx) + \".png\"\n",
    "        path_3 = self.carpeta_imagenes + \"Corner2/\" + str(idx) + \".png\"\n",
    "        path_4 = self.carpeta_imagenes + \"Corner3/\" + str(idx) + \".png\"\n",
    "        path_5 = self.carpeta_imagenes + \"Gripper/\" + str(idx) + \".png\"\n",
    "        path_6 = self.carpeta_imagenes + \"BehindGripper/\" + str(idx) + \".png\"\n",
    "        \n",
    "        im1 = np.array(Image.open(path_1))\n",
    "        im2 = np.array(Image.open(path_2))\n",
    "        im3 = np.array(Image.open(path_3))\n",
    "        im4 = np.array(Image.open(path_4))\n",
    "        im5 = np.array(Image.open(path_5))\n",
    "        im6 = np.array(Image.open(path_6))\n",
    "        \n",
    "        # TODO: get the action\n",
    "        data = pd.read_csv(self.archivo_acciones, header = None)\n",
    "        action = np.float32(np.array(data.iloc[idx]))\n",
    "        \n",
    "        if (idx % 25 == 0):\n",
    "            \n",
    "            action_prev_1 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_2 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_3 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_4 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_5 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            \n",
    "            im_t = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_1 = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_2 = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_3 = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_4 = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_5 = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            \n",
    "        elif ((idx-1) % 25 == 0):\n",
    "            action_prev_1 = np.float32(np.array(data.iloc[idx-1]))\n",
    "            action_prev_2 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_3 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_4 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_5 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            \n",
    "            im7 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-1) + \".png\"))\n",
    "            im8 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-1) + \".png\"))\n",
    "            im9 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-1) + \".png\"))\n",
    "            im10 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-1) + \".png\"))\n",
    "            im11 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-1) + \".png\"))\n",
    "            im12 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-1) + \".png\"))\n",
    "            \n",
    "            im_t = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_1 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_2 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_3 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_4 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_5 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            \n",
    "        elif ((idx-2) % 25 == 0):\n",
    "            action_prev_1 = np.float32(np.array(data.iloc[idx-1]))\n",
    "            action_prev_2 = np.float32(np.array(data.iloc[idx-2]))\n",
    "            action_prev_3 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_4 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_5 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            \n",
    "            im7 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-1) + \".png\"))\n",
    "            im8 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-1) + \".png\"))\n",
    "            im9 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-1) + \".png\"))\n",
    "            im10 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-1) + \".png\"))\n",
    "            im11 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-1) + \".png\"))\n",
    "            im12 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-1) + \".png\"))\n",
    "            \n",
    "            im13 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-2) + \".png\"))\n",
    "            im14 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-2) + \".png\"))\n",
    "            im15 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-2) + \".png\"))\n",
    "            im16 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-2) + \".png\"))\n",
    "            im17 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-2) + \".png\"))\n",
    "            im18 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-2) + \".png\"))\n",
    "            \n",
    "            im_t = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_1 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_2 = np.concatenate((im13, im14, im15, im16, im17, im18), axis=2)\n",
    "            im_prev_3 = np.concatenate((im13, im14, im15, im16, im17, im18), axis=2)\n",
    "            im_prev_4 = np.concatenate((im13, im14, im15, im16, im17, im18), axis=2)\n",
    "            im_prev_5 = np.concatenate((im13, im14, im15, im16, im17, im18), axis=2)\n",
    "            \n",
    "        elif ((idx-3) % 25 == 0):\n",
    "            action_prev_1 = np.float32(np.array(data.iloc[idx-1]))\n",
    "            action_prev_2 = np.float32(np.array(data.iloc[idx-2]))\n",
    "            action_prev_3 = np.float32(np.array(data.iloc[idx-3]))\n",
    "            action_prev_4 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            action_prev_5 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            \n",
    "            im7 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-1) + \".png\"))\n",
    "            im8 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-1) + \".png\"))\n",
    "            im9 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-1) + \".png\"))\n",
    "            im10 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-1) + \".png\"))\n",
    "            im11 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-1) + \".png\"))\n",
    "            im12 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-1) + \".png\"))\n",
    "            \n",
    "            im13 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-2) + \".png\"))\n",
    "            im14 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-2) + \".png\"))\n",
    "            im15 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-2) + \".png\"))\n",
    "            im16 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-2) + \".png\"))\n",
    "            im17 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-2) + \".png\"))\n",
    "            im18 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-2) + \".png\"))\n",
    "            \n",
    "            im19 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-3) + \".png\"))\n",
    "            im20 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-3) + \".png\"))\n",
    "            im21 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-3) + \".png\"))\n",
    "            im22 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-3) + \".png\"))\n",
    "            im23 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-3) + \".png\"))\n",
    "            im24 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-3) + \".png\"))\n",
    "            \n",
    "            im_t = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_1 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_2 = np.concatenate((im13, im14, im15, im16, im17, im18), axis=2)\n",
    "            im_prev_3 = np.concatenate((im19, im20, im21, im22, im23, im24), axis=2)\n",
    "            im_prev_4 = np.concatenate((im19, im20, im21, im22, im23, im24), axis=2)\n",
    "            im_prev_5 = np.concatenate((im19, im20, im21, im22, im23, im24), axis=2)\n",
    "            \n",
    "        elif ((idx-4) % 25 == 0):\n",
    "            action_prev_1 = np.float32(np.array(data.iloc[idx-1]))\n",
    "            action_prev_2 = np.float32(np.array(data.iloc[idx-2]))\n",
    "            action_prev_3 = np.float32(np.array(data.iloc[idx-3]))\n",
    "            action_prev_4 = np.float32(np.array(data.iloc[idx-4]))\n",
    "            action_prev_5 = np.array([0,0,0,0], dtype=\"float32\")\n",
    "            \n",
    "            im7 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-1) + \".png\"))\n",
    "            im8 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-1) + \".png\"))\n",
    "            im9 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-1) + \".png\"))\n",
    "            im10 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-1) + \".png\"))\n",
    "            im11 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-1) + \".png\"))\n",
    "            im12 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-1) + \".png\"))\n",
    "            \n",
    "            im13 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-2) + \".png\"))\n",
    "            im14 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-2) + \".png\"))\n",
    "            im15 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-2) + \".png\"))\n",
    "            im16 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-2) + \".png\"))\n",
    "            im17 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-2) + \".png\"))\n",
    "            im18 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-2) + \".png\"))\n",
    "            \n",
    "            im19 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-3) + \".png\"))\n",
    "            im20 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-3) + \".png\"))\n",
    "            im21 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-3) + \".png\"))\n",
    "            im22 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-3) + \".png\"))\n",
    "            im23 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-3) + \".png\"))\n",
    "            im24 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-3) + \".png\"))\n",
    "            \n",
    "            im25 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-4) + \".png\"))\n",
    "            im26 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-4) + \".png\"))\n",
    "            im27 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-4) + \".png\"))\n",
    "            im28 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-4) + \".png\"))\n",
    "            im29 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-4) + \".png\"))\n",
    "            im30 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-4) + \".png\"))\n",
    "            \n",
    "            im_t = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_1 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_2 = np.concatenate((im13, im14, im15, im16, im17, im18), axis=2)\n",
    "            im_prev_3 = np.concatenate((im19, im20, im21, im22, im23, im24), axis=2)\n",
    "            im_prev_4 = np.concatenate((im25, im26, im27, im28, im29, im30), axis=2)\n",
    "            im_prev_5 = np.concatenate((im25, im26, im27, im28, im29, im30), axis=2)\n",
    "            \n",
    "        else:\n",
    "            action_prev_1 = np.float32(np.array(data.iloc[idx-1]))\n",
    "            action_prev_2 = np.float32(np.array(data.iloc[idx-2]))\n",
    "            action_prev_3 = np.float32(np.array(data.iloc[idx-3]))\n",
    "            action_prev_4 = np.float32(np.array(data.iloc[idx-4]))\n",
    "            action_prev_5 = np.float32(np.array(data.iloc[idx-5]))\n",
    "            \n",
    "            im7 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-1) + \".png\"))\n",
    "            im8 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-1) + \".png\"))\n",
    "            im9 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-1) + \".png\"))\n",
    "            im10 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-1) + \".png\"))\n",
    "            im11 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-1) + \".png\"))\n",
    "            im12 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-1) + \".png\"))\n",
    "            \n",
    "            im13 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-2) + \".png\"))\n",
    "            im14 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-2) + \".png\"))\n",
    "            im15 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-2) + \".png\"))\n",
    "            im16 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-2) + \".png\"))\n",
    "            im17 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-2) + \".png\"))\n",
    "            im18 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-2) + \".png\"))\n",
    "            \n",
    "            im19 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-3) + \".png\"))\n",
    "            im20 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-3) + \".png\"))\n",
    "            im21 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-3) + \".png\"))\n",
    "            im22 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-3) + \".png\"))\n",
    "            im23 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-3) + \".png\"))\n",
    "            im24 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-3) + \".png\"))\n",
    "            \n",
    "            im25 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-4) + \".png\"))\n",
    "            im26 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-4) + \".png\"))\n",
    "            im27 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-4) + \".png\"))\n",
    "            im28 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-4) + \".png\"))\n",
    "            im29 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-4) + \".png\"))\n",
    "            im30 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-4) + \".png\"))\n",
    "            \n",
    "            im31 = np.array(Image.open(self.carpeta_imagenes + \"Top/\" + str(idx-5) + \".png\"))\n",
    "            im32 = np.array(Image.open(self.carpeta_imagenes + \"Corner/\" + str(idx-5) + \".png\"))\n",
    "            im33 = np.array(Image.open(self.carpeta_imagenes + \"Corner2/\" + str(idx-5) + \".png\"))\n",
    "            im34 = np.array(Image.open(self.carpeta_imagenes + \"Corner3/\" + str(idx-5) + \".png\"))\n",
    "            im35 = np.array(Image.open(self.carpeta_imagenes + \"Gripper/\" + str(idx-5) + \".png\"))\n",
    "            im36 = np.array(Image.open(self.carpeta_imagenes + \"BehindGripper/\" + str(idx-5) + \".png\"))\n",
    "            \n",
    "            im_t = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "            im_prev_1 = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "            im_prev_2 = np.concatenate((im13, im14, im15, im16, im17, im18), axis=2)\n",
    "            im_prev_3 = np.concatenate((im19, im20, im21, im22, im23, im24), axis=2)\n",
    "            im_prev_4 = np.concatenate((im25, im26, im27, im28, im29, im30), axis=2)\n",
    "            im_prev_5 = np.concatenate((im31, im32, im33, im34, im35, im36), axis=2)\n",
    "            \n",
    "        #im_actual = np.concatenate((im1, im2, im3, im4, im5, im6), axis=2)\n",
    "        #im_previa = np.concatenate((im7, im8, im9, im10, im11, im12), axis=2)\n",
    "        #imagen = np.concatenate((im_t, im_prev_1), axis=2) # Canales = 2, Anchura = 1 -> 1,1,1...1,2\n",
    "        # Return both image and action\n",
    "        return im_t, im_prev_1, im_prev_2, im_prev_3, im_prev_4, im_prev_5, action, action_prev_1, action_prev_2, action_prev_3, action_prev_4, action_prev_5\n",
    "\n",
    "class ObsActionDatasetTrain(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for getting the data. In this case, from ObsActionGetter object\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_action_getter: ObsActionGetter, transformations: List[Callable]):\n",
    "        super(ObsActionDatasetTrain, self).__init__()\n",
    "        self.obs_action_getter = obs_action_getter\n",
    "        self.transformations = transformations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.obs_action_getter)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im_t, im_prev_1, im_prev_2, im_prev_3, im_prev_4, im_prev_5, action, action_prev_1, action_prev_2, action_prev_3, action_prev_4, action_prev_5 = self.obs_action_getter[idx]\n",
    "        for t in self.transformations:\n",
    "            im_t = t(im_t)\n",
    "            im_prev_1 = t(im_prev_1)\n",
    "            im_prev_2 = t(im_prev_2)\n",
    "            im_prev_3 = t(im_prev_3)\n",
    "            im_prev_4 = t(im_prev_4)\n",
    "            im_prev_5 = t(im_prev_5)\n",
    "        # Return both image and action\n",
    "        return (im_t, im_prev_1, im_prev_2, im_prev_3, im_prev_4, im_prev_5, action_prev_1, action_prev_2, action_prev_3, action_prev_4, action_prev_5), action - action_prev_1\n",
    "\n",
    "class ObsActionDatasetTest(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for getting the data. In this case, from ObsActionGetter object\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_action_getter: ObsActionGetter, transformations: List[Callable]):\n",
    "        super(ObsActionDatasetTest, self).__init__()\n",
    "        self.obs_action_getter = obs_action_getter\n",
    "        self.transformations = transformations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.obs_action_getter)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im_t, im_prev_1, im_prev_2, im_prev_3, im_prev_4, im_prev_5, action, action_prev_1, action_prev_2, action_prev_3, action_prev_4, action_prev_5 = self.obs_action_getter[idx]\n",
    "        for t in self.transformations:\n",
    "            im_t = t(im_t)\n",
    "            im_prev_1 = t(im_prev_1)\n",
    "            im_prev_2 = t(im_prev_2)\n",
    "            im_prev_3 = t(im_prev_3)\n",
    "            im_prev_4 = t(im_prev_4)\n",
    "            im_prev_5 = t(im_prev_5)\n",
    "        # Return both image and action\n",
    "        return (im_t, im_prev_1, im_prev_2, im_prev_3, im_prev_4, im_prev_5, action_prev_1, action_prev_2, action_prev_3, action_prev_4, action_prev_5), action\n",
    "    \n",
    "class MultiImage(nn.Module):\n",
    "    def __init__(self, fe, clf, lstm, num_layers, hidden):\n",
    "        super(MultiImage, self).__init__()\n",
    "        self.fe = fe\n",
    "        self.clf = clf\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lstm = lstm\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden = hidden\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11 = x\n",
    "        \n",
    "        #LSTM\n",
    "        f1 = self.flatten(self.avg_pool(self.fe(x1)))\n",
    "        f2 = self.flatten(self.avg_pool(self.fe(x2)))\n",
    "        f3 = self.flatten(self.avg_pool(self.fe(x3)))\n",
    "        f4 = self.flatten(self.avg_pool(self.fe(x4)))\n",
    "        f5 = self.flatten(self.avg_pool(self.fe(x5)))\n",
    "        f6 = self.flatten(self.avg_pool(self.fe(x6)))\n",
    "        \n",
    "        aux = [torch.cat((f2, x7), 1), torch.cat((f3, x8), 1), torch.cat((f4, x9), 1), torch.cat((f5, x10), 1), torch.cat((f6, x11), 1)]\n",
    "        \n",
    "        inputs = torch.zeros(len(aux), np.shape(aux[0])[0], np.shape(aux[0])[1])\n",
    "        for i in range(np.shape(inputs)[0]):\n",
    "            inputs[i] = aux[i]\n",
    "        if np.shape(aux[0])[0] == 32:\n",
    "            xlstm = self.lstm(inputs.cuda(), self.hidden)[0][-1]\n",
    "        else: \n",
    "            hidden = (torch.randn(self.num_layers, np.shape(aux[0])[0], np.shape(aux[0])[1]).cuda(), torch.randn(self.num_layers, np.shape(aux[0])[0], np.shape(aux[0])[1]).cuda())\n",
    "            xlstm = self.lstm(inputs.cuda(), hidden)[0][-1]\n",
    "        \n",
    "        xlstm = torch.squeeze(xlstm)\n",
    "        f = torch.cat((f1, xlstm), dim=1)\n",
    "        return self.clf(f)+x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0abc4508",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10500\n",
      "10500\n",
      "10500\n",
      "10500\n",
      "10500\n",
      "10500\n",
      "42000\n",
      "42000\n",
      "42000\n",
      "42000\n",
      "42000\n",
      "42000\n"
     ]
    }
   ],
   "source": [
    "print(len(fnmatch.filter(os.listdir('Test_dataset/Top'), '*.png')))\n",
    "print(len(fnmatch.filter(os.listdir('Test_dataset/Corner'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Test_dataset/Corner2'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Test_dataset/Corner3'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Test_dataset/Gripper'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Test_dataset/BehindGripper'), '*')))\n",
    "\n",
    "print(len(fnmatch.filter(os.listdir('Train_dataset/Top'), '*.png')))\n",
    "print(len(fnmatch.filter(os.listdir('Train_dataset/Corner'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Train_dataset/Corner2'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Train_dataset/Corner3'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Train_dataset/Gripper'), '*')))\n",
    "print(len(fnmatch.filter(os.listdir('Train_dataset/BehindGripper'), '*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b677b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para mas de una imagen\n",
    "\n",
    "# Create train Dataset.\n",
    "# The ToTensor transform converts the image to Tensor in [0, 1] range and makes it channel first\n",
    "# The Normalize transform normalizes the tensor using Imagenet stats\n",
    "\n",
    "train_obs_action_getter = ObsActionGetter('Train_dataset/', 'Train_dataset/Train_Actions.csv')\n",
    "train_dataset = ObsActionDatasetTrain(train_obs_action_getter, \n",
    "                                [\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.485, 0.456, 0.406, 0.485, 0.456, 0.406, 0.485, 0.456, 0.406, \n",
    "                                                          0.485, 0.456, 0.406, 0.485, 0.456, 0.406, 0.485, 0.456, 0.406),\n",
    "                                                         (0.229, 0.224, 0.225, 0.229, 0.224, 0.225, 0.229, 0.224, 0.225,\n",
    "                                                          0.229, 0.224, 0.225, 0.229, 0.224, 0.225, 0.229, 0.224, 0.225))\n",
    "                                ])\n",
    "\n",
    "# Create test Dataset.\n",
    "# The ToTensor transform converts the image to Tensor in [0, 1] range and makes it channel first\n",
    "# The Normalize transform normalizes the tensor using Imagenet stats\n",
    "\n",
    "test_obs_action_getter = ObsActionGetter('Test_dataset/', 'Test_dataset/Test_Actions.csv')\n",
    "test_dataset = ObsActionDatasetTest(test_obs_action_getter, \n",
    "                                [\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.485, 0.456, 0.406, 0.485, 0.456, 0.406, 0.485, 0.456, 0.406, \n",
    "                                                          0.485, 0.456, 0.406, 0.485, 0.456, 0.406, 0.485, 0.456, 0.406),\n",
    "                                                         (0.229, 0.224, 0.225, 0.229, 0.224, 0.225, 0.229, 0.224, 0.225,\n",
    "                                                          0.229, 0.224, 0.225, 0.229, 0.224, 0.225, 0.229, 0.224, 0.225))\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af32bd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000\n",
      "1312\n",
      "10500\n",
      "329\n",
      "CPU times: user 134 ms, sys: 61.2 ms, total: 195 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create loaders (class that groups examples in batches)\n",
    "# Uriz: train DataLoader should have shuffle to True\n",
    "# Uriz: test DataLoader should have drop_last to False\n",
    "# Uriz: use more workers to improve the training speed (load data in sever CPU threads). Otherwise we have a CPU -> GPU bottleneck\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, num_workers=16)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a4ed374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(30,34):\\n    print(\"\\nAccion Actual:\", train_obs_action_getter[i][1])\\n    print(\"Accion Previa - 1:\", train_obs_action_getter[i][2])\\n    print(\"Accion Previa - 2:\", train_obs_action_getter[i][3])\\n    print(\"Accion Previa - 3:\", train_obs_action_getter[i][4])\\n    print(\"Accion Previa - 4:\", train_obs_action_getter[i][5])\\n    print(\"Accion Previa - 5:\", train_obs_action_getter[i][6])\\n    '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(30,34):\n",
    "    print(\"\\nAccion Actual:\", train_obs_action_getter[i][1])\n",
    "    print(\"Accion Previa - 1:\", train_obs_action_getter[i][2])\n",
    "    print(\"Accion Previa - 2:\", train_obs_action_getter[i][3])\n",
    "    print(\"Accion Previa - 3:\", train_obs_action_getter[i][4])\n",
    "    print(\"Accion Previa - 4:\", train_obs_action_getter[i][5])\n",
    "    print(\"Accion Previa - 5:\", train_obs_action_getter[i][6])\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bba12f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f64a607b650>\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_loader)\n",
    "first = next(it)\n",
    "second = next(it)\n",
    "print(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3716f2",
   "metadata": {},
   "source": [
    "\n",
    "## Model\n",
    "\n",
    "We use a pretrained ResNet18 for now. We have to remove the classifier head because it is traiend for classification with 1000 classes. We add a custom head to make regression of 7 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d898c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pretrain\n",
    "model = models.resnet18(pretrained=False)\n",
    "\n",
    "# Change the first convolution\n",
    "new_conv = torch.nn.Conv2d(18, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model = list(model.children())[1:]\n",
    "model = [new_conv] + model\n",
    "model = torch.nn.Sequential(*model)\n",
    "\n",
    "# Get only the feature extractor (remove avgpool and fc layers)\n",
    "fe = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "def freeze_all_but_bn(m):\n",
    "    \"\"\"\n",
    "    Function that set a module as no trainable (not required grad) only if it is not a BatchNorm module\n",
    "    Args:\n",
    "        m: PyTorch Module\n",
    "    \"\"\"\n",
    "    if not isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            m.weight.requires_grad_(False)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.requires_grad_(False)\n",
    "            \n",
    "def unfreeze_all_but_bn(m):\n",
    "    \"\"\"\n",
    "    Function that set a module as no trainable (not required grad) only if it is not a BatchNorm module\n",
    "    Args:\n",
    "        m: PyTorch Module\n",
    "    \"\"\"\n",
    "    if not isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            m.weight.requires_grad_(True)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            m.bias.requires_grad_(True)\n",
    "            \n",
    "# Freeze all but BatchNorm layers of feature extractor\n",
    "# Uriz: if we use the \"normal\" ResNet use the pretained version and freeze all but BN\n",
    "#       only set all trainiable when we change the fitst convolution layer to accept images of 6 dimensions\n",
    "#       if the ResNet is pretrained and freezed the training accuracy is maintained (or increased) and the train time is dreceased\n",
    "fe.apply(freeze_all_but_bn) # Solo usar si no se preentrena la red\n",
    "\n",
    "# Create custom head\n",
    "head_clf = nn.Sequential(\n",
    "    nn.BatchNorm1d(512+512+4),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(512+512+4, 512), \n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),   \n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(512, 4)\n",
    ")\n",
    "\n",
    "num_layers = 10\n",
    "dropout = 0.2\n",
    "bidirectional = False\n",
    "hidden = (torch.randn(num_layers, BATCH_SIZE, 512+4).cuda(), torch.randn(num_layers, BATCH_SIZE, 512+4).cuda())\n",
    "lstm = lstm = nn.LSTM(np.shape(hidden[0])[2], np.shape(hidden[0])[2], num_layers = num_layers, bidirectional = bidirectional, dropout = dropout)\n",
    "\n",
    "# Rebuild the model\n",
    "model = MultiImage(fe, head_clf, lstm, num_layers, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079655b",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "523eed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainable class with Pytorch Lightinng\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, loss_function, total_iterations, lr):\n",
    "        \"\"\"\n",
    "        Constructor of the trainable class\n",
    "        Args:\n",
    "            model: PyTorch model\n",
    "            loss_function: The loss function to use\n",
    "            total_iterations: The total number of iterations for training (num_batches * num_epochs). Used for\n",
    "                                LR Schedule\n",
    "            lr: the max_lr to use for the OneCycleLR policy\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Store params\n",
    "        self.model = model\n",
    "        self.loss_function = loss_function\n",
    "        self.total_iterations = total_iterations\n",
    "        self.lr = lr\n",
    "        # For measure the MSE\n",
    "        self.train_mse = torchmetrics.MeanSquaredError()\n",
    "        self.valid_mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward method\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_function(prediction, y)\n",
    "        # Get MSE\n",
    "        self.train_mse(prediction, y)\n",
    "        # Log loss and MSE\n",
    "        self.log(\"training_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('train_mse', self.train_mse, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        # Return Loss for backward\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.log('train_acc_epoch', self.train_mse.compute(), logger=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = self.loss_function(prediction, y)\n",
    "        # Get MSE\n",
    "        self.valid_mse(prediction, y)\n",
    "        # Log loss and MSE\n",
    "        self.log('valid_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('val_mse', self.valid_mse, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        # log epoch metric\n",
    "        self.log('val_acc_epoch', self.valid_mse.compute(), logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Get Adam optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        # Set OneCycleLR policy\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.lr,\n",
    "                                                             total_steps=self.total_iterations),\n",
    "            'interval': 'step', 'frequency': 1, 'name': 'lr_logger'\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f13c6abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackClass(Callback):\n",
    "    def __init__(self, what=\"epochs\", verbose=True):\n",
    "        self.what = what\n",
    "        self.verbose = verbose\n",
    "        self.state = {\"epochs\": 0}\n",
    "        \n",
    "    @property\n",
    "    def state_key(self):\n",
    "        # note: we do not include `verbose` here on purpose\n",
    "        return self._generate_state_key(what=self.what)\n",
    "\n",
    "    def on_train_epoch_end(self, *args, **kwargs):\n",
    "        e = 0\n",
    "        if self.what == \"epochs\":\n",
    "            self.state[\"epochs\"] += 1\n",
    "            \n",
    "        if self.state[\"epochs\"] >= 5:\n",
    "            for param in pl_model.parameters():\n",
    "                param.requires_grad = True # Unfreeze\n",
    "        \n",
    "        for name, param in pl_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                e = e+1\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.state.update(state_dict)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.state.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fca76d",
   "metadata": {},
   "source": [
    "## Bucle to obtain all the info for differents Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5878bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_List = []\n",
    "Model_List = []\n",
    "Prediction_List = []\n",
    "Label_List = []\n",
    "LR_List = []\n",
    "Loss_List = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc2771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44950d826a6046369c483c68cb482e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 6.58E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEMCAYAAADJQLEhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuEklEQVR4nO3deXyU5bn4/8+VfSWQkEAgCWEH2RIIu4JoRUUrVqGK2hatUnvaWr/2eKpdXI7tqednta311Kqt1VorKi5FwBYXUEQBw77vS0ICSQjZ15m5fn/MkGIIMYHMTDJzvV+veWXmee55nit5Xplr7uW5b1FVjDHGBK8QfwdgjDHGvywRGGNMkLNEYIwxQc4SgTHGBDlLBMYYE+QsERhjTJDzeiIQkVAR2SgiS1rYFykir4rIPhFZKyKZ3o7HGGPMF/miRvBDYOdZ9n0bOKmqg4DfAP/rg3iMMcacJsybBxeRNOAq4JfAPS0UmQ085Hm+CHhKRERbucutZ8+empmZ2cGRGmNMYFu/fn2Jqia3tM+riQD4LfBfQPxZ9vcF8gBU1SEi5UASUHJ6IRFZACwAyMjIIDc311vxGmNMQBKRw2fb57WmIRG5GihS1fXneyxVfVZVc1Q1Jzm5xYRmjDHmHHmzj2AqcI2IHAIWApeIyN+alTkKpAOISBiQAJzwYkzGGGOa8VoiUNX7VTVNVTOBG4EPVfWWZsUWA9/yPJ/jKWOz4BljjA95u4/gDCLy30Cuqi4G/gy8JCL7gFLcCaPdGhsbyc/Pp66urgMjNZ1RVFQUaWlphIeH+zsUYwKGdLUv4Dk5Odq8s/jgwYPEx8eTlJSEiPgpMuNtqsqJEyeorKykf//+/g7HmC5FRNarak5L+wLizuK6ujpLAkFAREhKSrKanzEdLCASAWBJIEjYdTbB6p/bjpFXWuOVYwdMImgXVVizBt56y/3TS81jv/3tb6mp8c6Fa6uysjL+8Ic/+Ox8mZmZlJS4bwOZMmXKOR/nhRdeoKCgoKPCMqZLq2lwcNcrG3lpzVlvBTgvwZcIli2DjAy47DKYP9/9MyPDvb2DBUoicDgc5/S+Tz/99JzPaYnAmH9bc+AEDU4X0wZ75z6q4EoEy5bBnDmQnw9VVVBR4f6Zn+/efo7JoLq6mquuuooxY8YwcuRIXn31VZ588kkKCgqYMWMGM2bMAGD58uVMnjyZsWPHMnfuXKqqqgBYv34906dPZ9y4cVx++eUUFhYCcPHFF/PDH/6QrKwsRo4cybp165rOd9tttzFhwgSys7P5xz/+AcD27duZMGECWVlZjB49mr1793Lfffexf/9+srKyuPfee8+I/ZFHHmHo0KFceOGFzJs3j1//+tdN57777rvJycnhd7/7He+88w4TJ04kOzubr3zlKxw/fhyAEydOMHPmTEaMGMHtt9/O6YMP4uLimp4/9thjjB8/ntGjR/Pggw8CcOjQIYYPH84dd9zBiBEjmDlzJrW1tSxatIjc3FxuvvlmsrKyqK2tPafrYkyg+HhPCVHhIeRk9vDOCVS1Sz3GjRunze3YseOMbWdwuVT79lV1NwS1/EhLc5drp0WLFuntt9/e9LqsrExVVfv166fFxcWqqlpcXKwXXXSRVlVVqarqo48+qg8//LA2NDTo5MmTtaioSFVVFy5cqLfeequqqk6fPr3puB999JGOGDFCVVXvv/9+femll1RV9eTJkzp48GCtqqrS73//+/q3v/1NVVXr6+u1pqZGDx482PS+5tatW6djxozR2tparaio0EGDBuljjz3WdO7vfve7TWVLS0vV5fnbPPfcc3rPPfeoquoPfvADffjhh1VVdcmSJQo0/c6xsbGqqvqvf/1L77jjDnW5XOp0OvWqq67Sjz76SA8ePKihoaG6ceNGVVWdO3du0+81ffp0/fzzz1uMu03X25gAMuPXK/Sbf157XsfAPWy/xc9Vn99H4Ddr10J5eetlyspg3TqYOLFdhx41ahQ/+tGP+PGPf8zVV1/NRRdddEaZNWvWsGPHDqZOnQpAQ0MDkydPZvfu3Wzbto3LLrsMAKfTSWpqatP75s2bB8C0adOoqKigrKyM5cuXs3jx4qZv73V1dRw5coTJkyfzy1/+kvz8fK677joGDx7catyrV69m9uzZREVFERUVxVe/+tUv7L/hhhuanufn53PDDTdQWFhIQ0ND0/DNjz/+mDfffBOAq666ih49zvzGsnz5cpYvX052djYAVVVV7N27l4yMDPr3709WVhYA48aN49ChQ63GbEywyT9Zw4Hiam6akOG1cwRPIigshJAvaQkLCYFzaJceMmQIGzZsYNmyZfzsZz/j0ksv5YEHHvhCGVXlsssu45VXXvnC9q1btzJixAg+++yzFo/dfJSMiKCqvPHGGwwdOvQL+4YPH87EiRNZunQps2bN4plnnmHAgAHt/n1OiY2NbXr+gx/8gHvuuYdrrrmGlStX8tBDD7X5OKrK/fffz3e+850vbD906BCRkZFNr0NDQ60ZyJhmVu11D76YPsR786wFTx9Baiq4XK2XcbmgT592H7qgoICYmBhuueUW7r33XjZs2ABAfHw8lZWVAEyaNInVq1ezb98+wN3Ov2fPHoYOHUpxcXFTImhsbGT79u1Nx3711VcB+OSTT0hISCAhIYHLL7+c3//+903t8Rs3bgTgwIEDDBgwgLvuuovZs2ezZcuWL8TQ3NSpU3nnnXeoq6ujqqqKJUvOWDuoSXl5OX379gXgxRdfbNo+bdo0/v73vwPw7rvvcvLkyTPee/nll/P888839YkcPXqUoqKiVv+mrcVtTDBZtbeY1IQoBqXEfXnhcxQ8NYKJEyEhwd05fDbdu8OECe0+9NatW7n33nsJCQkhPDycp59+GoAFCxZwxRVX0KdPH1asWMELL7zAvHnzqK+vB+AXv/gFQ4YMYdGiRdx1112Ul5fjcDi4++67GTFiBOCeUiE7O5vGxkaef/55AH7+859z9913M3r0aFwuF/3792fJkiW89tprvPTSS4SHh9O7d29+8pOfkJiYyNSpUxk5ciRXXnkljz32WFPc48eP55prrmH06NH06tWLUaNGkZCQ0OLv+NBDDzF37lx69OjBJZdcwsGDBwF48MEHmTdvHiNGjGDKlClkZJxZfZ05cyY7d+5k8uTJgLsT+W9/+xuhoaFn/ZvOnz+fO++8k+joaD777DOio6Pbe1mM6fIcThef7C3hipG9vXsPzdk6Dzrr45w7i1VVly5VjY5uuaM4Otq9vxNprcO0o1RWVqqqanV1tY4bN07Xr1/v1fN1BOssNsFi/eFS7ffjJfrO5qPnfSxa6SwOnqYhgFmzYNEiSEuDuDjo1s39My3NvX3WLH9H6HMLFiwgKyuLsWPHcv311zN27Fh/h2SM8fh4TzEiMHVgT6+eJ3iahk6ZNQuOHHGPDioocPcJTJgAnXDqgpUrV3r9HKfa940xnc/He4oZndadHrERXj1P8CUCcH/ot3OIqDHG+FJ5bSOb8sr43oxBXj9XwDQNaRebTtucG7vOJlh8uq8El8I0Lw4bPSUgEkFUVBQnTpywD4kAp571CKKiovwdijFe9/HeEuIjw8hK7+71cwVE01BaWhr5+fkUFxf7OxTjZadWKDMm0K3aW8zkgUmEh3r/+3pAJILw8HBbscoYEzBqG5zkn6xlnhenlThdQDQNGWNMIDlW4V6Fr3c33zSDWiIwxphO5li5OxGkJlgiMMaYoHSswj35Yi9LBMYYE5yOlbvnI7OmIWOMCVLHymuJjwojNtI343ksERhjTCdzrKLOZ7UBsERgjDGdzrGKenr7qH8ALBEYY0ync6y81moExhgTrBxOF8WVViMwxpigVVxVj0uxRGCMMcHq1M1kAdE0JCJRIrJORDaLyHYRebiFMvNFpFhENnket3srHmOM6QqOn5pewoc1Am8OUq0HLlHVKhEJBz4RkXdVdU2zcq+q6ve9GIcxxnQZhX6oEXgtEXgWS67yvAz3PGzBAGOMacWxijoiQkNI9PLylKfzah+BiISKyCagCHhPVde2UOx6EdkiIotEJN2b8RhjTGd3rLyOXgmRiA/XUfdqIlBVp6pmAWnABBEZ2azIO0Cmqo4G3gNebOk4IrJARHJFJNcWnzHGBLJj5b69qxh8NGpIVcuAFcAVzbafUNV6z8s/AePO8v5nVTVHVXOSk72/fqcxxvjL8Yo6eidE+/Sc3hw1lCwi3T3Po4HLgF3NyqSe9vIaYKe34jHGmM5OVSksr6N3t0ifntebo4ZSgRdFJBR3wnlNVZeIyH8Duaq6GLhLRK4BHEApMN+L8RhjTKdWXttIvcNFLx83DXlz1NAWILuF7Q+c9vx+4H5vxWCMMV1JYdPKZAHSNGSMMaZ9mtYqTvBt05AlAmOM6SSOn7qZzGoExhgTnArL6xCBlHirERhjTFA6XlFHUmwk4aG+/Wi2RGCMMZ1EYXkdqT6cbO4USwTGGNNJHK+o8/nQUbBEYIwxnYbVCIwxJojVNTopr2306ToEp1giMMaYTuDUymTWNGSMMUHq33cVWyIwxpigdGqJSqsRGGNMkGpaotJqBMYYE5yOV9QRHxlGXKQ3J4VumSUCY4zpBNxLVPq+NgCWCIwxplMorPDPPQRgicAYYzqF4+X+uasYLBEYY4zf1TY4Kaqso093304/fYolAmOM8bNtBeW4FEb3TfDL+S0RGGOMn23OKwNgdLolAmOMCUqb88vpkxBFSrz1ERhjTFDanFfGmPTufju/JQJjjPGjk9UNHCmtYXRad7/FYInAGGP8aHN+GQBj/NQ/AJYIjDHGrzbnlSMCo/w0YggsERhjjF9tyS9jYHIc8VHhfovBEoExxviJqrI5v4wxfuwfAEsExhjjN0fLaimpaiDLj/0DYInAGGP8Zkt+OYBfRwyBJQJjjPGbzXllRISGMCw13q9xWCIwxhg/2ZRXxvA+3YgMC/VrHF5LBCISJSLrRGSziGwXkYdbKBMpIq+KyD4RWSsimd6KxxhjOhOnS9l2tJwxaf7tHwDv1gjqgUtUdQyQBVwhIpOalfk2cFJVBwG/Af7Xi/EYY0ynsb+4iuoGp99HDIEXE4G6VXlehnse2qzYbOBFz/NFwKUiIt6KyRhjOotNnhlH/TnH0Cle7SMQkVAR2QQUAe+p6tpmRfoCeQCq6gDKgaQWjrNARHJFJLe4uNibIRtjjE9syS8jPjKMAT1j/R2KdxOBqjpVNQtIAyaIyMhzPM6zqpqjqjnJyckdGqMxxvjD5rxyRqUlEBLi/0YQn4waUtUyYAVwRbNdR4F0ABEJAxKAE76IyRhj/KWyrpGdhRWdolkIvDtqKFlEunueRwOXAbuaFVsMfMvzfA7woao270cwxpiAsmpvCQ6XcvGQztHCEebFY6cCL4pIKO6E85qqLhGR/wZyVXUx8GfgJRHZB5QCN3oxHmOM6RTe33mchOhwxvXr4e9QAC8mAlXdAmS3sP2B057XAXO9FYMxxnQ2TpeyYlcRM4YmExbaOe7p7RxRGGNMkNhw5CQnaxr5ygW9/B1KE0sExhjjQ+/vPE5YiDCtk/QPgCUCY4zxqfd3HGfigES6+XEhmuYsERhjjI8cKqlmf3E1XxneeZqFwBKBMcb4zPs7jwNYIjDGmGD1wc4ihvSKIz0xxt+hfIElAmOM8YHymkbWHSrtdLUBsERgjDE+sXJPEU6XcqklAmOMCU4f7CwiKTaCrE4yv9DpLBEYY4yXHa+oc99NPCyF0E4w22hzlgiMMcaLGp0uvvfyBhwu5TvTBvg7nBZ5c9I5Y4wJer9atovcwyd5cl42g3vF+zucFlmNwBhjvGTJlgKeX32Q+VMyuWZMH3+Hc1aWCIwxpgM4nC4cTlfT673HK/mvRVsY168HP5k13I+RfTlrGjLGmPPU4HAx/bEVFJbXEREWQmxEKA0OFzERofzfTWOJCOvc37ktERhjzHnalFdGYXkd149NIzk+kpoGB/WNLm6Z1I/eCVH+Du9LtSkRiEgsUKuqLhEZAgwD3lXVRq9GZ4wxXcAn+0oIEXjg6gtIiOk8s4q2VVvrKx8DUSLSF1gOfAN4wVtBGWNMV7J6Xwmj0rp3ySQAbU8Eoqo1wHXAH1R1LjDCe2EZY0zXUFnXyKa8Mi4clOTvUM5ZmxOBiEwGbgaWeraFeickY4zpOtYeKMXpUqYO6unvUM5ZWxPB3cD9wFuqul1EBgArvBaVMcZ0EZ/sKyEqPISxGT38Hco5a1Nnsap+BHwEICIhQImq3uXNwIwxpitYva+E8ZmJRIV33UaSNtUIROTvItLNM3poG7BDRO71bmjGGNO5Ha+oY29RFRd24WYhaHvT0AWqWgFcC7wL9Mc9csgYY4LWp/tLALp0/wC0PRGEi0g47kSw2HP/gHotKmOM6QI+2XuCHjHhXJDazd+hnJe2JoJngENALPCxiPQDKrwVlDHGdHaqyup9JUwZ1JOQTrjGQHu0tbP4SeDJ0zYdFpEZ3gnJGGM6v/3F1RyrqOvy/QPQ9s7iBBF5QkRyPY/HcdcOjDEmKK3e5+4fCJpEADwPVAJf9zwqgL94KyhjjOnsPtlXQkZiDOmJMf4O5by1NREMVNUHVfWA5/Ew0OqaayKSLiIrRGSHiGwXkR+2UOZiESkXkU2exwPn8ksYY4wv1TucrNl/gqldeFqJ07V1GupaEblQVT8BEJGpQO2XvMcB/EhVN4hIPLBeRN5T1R3Nyq1S1avbF7YxxvjPBzuLqKx3cMXIVH+H0iHamgjuBP4qIgme1yeBb7X2BlUtBAo9zytFZCfQF2ieCIwxpkt5LTeP1ISogOgfgDY2DanqZlUdA4wGRqtqNnBJW08iIplANrC2hd2TRWSziLwrIi3OaCoiC051VBcXF7f1tMYY0+EKy2v5eE8xc8alEdrFh42e0q7101S1wnOHMcA9bXmPiMQBbwB3n/beUzYA/TxJ5vfA22c577OqmqOqOcnJye0J2RhjOtSbG47iUpgzLs3foXSY81lI80tToedu5DeAl1X1zeb7PYmlyvN8Ge47mAOjrmWMCTiqymu5eUwakEi/pMAZQX8+iaDVKSZERIA/AztV9YmzlOntKYeITPDEc+I8YjLGGK9Zd7CUwydqmDsu3d+hdKhWO4tFpJKWP/AFiP6SY0/FPTHdVhHZ5Nn2EyADQFX/CMwBvisiDtyjkG5UVZvDyBjTKb2+Pp+4yDCuHNXb36F0qFYTgarGn+uBPUNNW20+UtWngKfO9RzGGOMrVfUOlm4p5NrsPsREtHXAZddwPk1DxhgTNJZuKaC20cncnMBqFgJLBMYY0yav5eYzKCWO7PTu/g6lw1kiMMaYL7H+8EnWHz7JjePT8YxvCSiWCIwx5ks88d5uesZFcNPEDH+H4hWWCIwxphVrDpxg9b4T3Dl9YMB1Ep9iicAYY85CVXli+R56dYvklkn9/B2O11giMMaYs/hkXwnrDpXyvRmDiAoP9Xc4XmOJwBhjWqCqPL58D30SorhhfOANGT2dJQJjjGnByt3FbMor4/uXDCYyLHBrA2CJwBhjzuB0KY+/t5v0xGjm5gTOLKNnY4nAGGOa+dOqA2w7WsF/zhxKeGjgf0wG/m9ojDHtsKOggl8v380VI3pzzZg+/g7HJywRGGOMR12jk3te20RCdAT/c92ogLyLuCWBeXeEMcacgyfe28OuY5X8Zf54EmMj/B2Oz1iNwBgTlGoaHJTVNNDodAHw2f4TPLfqADdPzGDGsBQ/R+dbViMwxgSdf2w6yn1vbKW20QlAVHgILhf0S4zhp1cN93N0vmeJwBgTNBqdLn61bBfPrz7I+MweXDkylap6B9X1DuodLm6ZlBGw8wm1Jvh+Y2NMUCqpqud7L29g7cFS5k/J5KdXDQ+KoaFtYYnAGBPwymsbmf3Uakqq6vnNDWP4Wnbg3yTWHpYIjDEB77fv76GgvJbXvzOZnMxEf4fT6Vi9yBgT0HYfq+Svnx1m3oQMSwJnYYnAGBOwVJUHF28jPiqMe2cO9Xc4nZYlAmNMwFqypZA1B0r5z5lD6RFEN4i1lyUCY0xAqq538D/LdjKiTzfmTQjMtYY7inUWG2MC0v+t2EdheR1P3ZRNaEhwzBl0rqxGYIwJOCVV9fxp1UGuy+7LuH7WQfxlLBEYYwLOovX5NDhd/MeMgf4OpUuwRGCMCSiqysJ1Rxif2YNBKfH+DqdLsERgjAkonx04waETNdZB3A5eSwQiki4iK0Rkh4hsF5EftlBGRORJEdknIltEZKy34jHGBIeF6/LoFhXGrFGp/g6ly/BmjcAB/EhVLwAmAd8TkQualbkSGOx5LACe9mI8xpgAd7K6gX9uO8Z1Y9OICg/1dzhdhtcSgaoWquoGz/NKYCfQt1mx2cBf1W0N0F1ELI0bY87JGxvcncQ3Tkj3dyhdik/6CEQkE8gG1jbb1RfIO+11PmcmC0RkgYjkikhucXGx1+I0xnRdqsrCz/PISu/OsN7d/B1Ol+L1RCAiccAbwN2qWnEux1DVZ1U1R1VzkpOTOzZAY0xAyD18kn1FVdxkncTt5tVEICLhuJPAy6r6ZgtFjgKn1+HSPNuMMaZdXll3hLjIMK4eY63L7eW1KSZERIA/AztV9YmzFFsMfF9EFgITgXJVLfRGPHWNTkqq6qmsc1BZ56CitpGQEJg2OJkwW6XImC7teEUdS7cUcv24tKBcavJ8efMvNhX4BrBVRDZ5tv0EyABQ1T8Cy4BZwD6gBrjVW8Es33Gcu17ZeMb2ob3iefCaC5gysGeHns/hdLE5v5xGp6tpW2RYCKP6JljiMaaDPb58Ny5VFlw0wN+hdEleSwSq+gnQ6kxPqqrA97wVw+my07vzv9ePIj4qnG5R4cRHhZF3soZH393FTc+t5apRqdw/axhpPWLadLzXc/P45bKdXDqsF9+Y3I+s9O6Au+bxxoZ8/vjRfvJKa894X8+4CGaNSmV2Vh/GZvTAXXEyxpyr7QXlvL4+n9sv7E9mz1h/h9MlifuzuOvIycnR3NzcDjteXaOTZz8+wB9W7qPRqWQmxTA4JZ7BveK4ILUbl13Q64xv8G+sz+c/F21mcEocR0/WUt3gZFTfBKYO6smbG/IpqqxnTFoCt13Yn+T4yKb3lVY38O7WY7y/8zj1Dhe9u0UxIDmW1IRo+nSPIr1HDJcOTyEpLrJ5mMaYFqgqN/9pLTsLK1h57wwSosP9HVKnJSLrVTWnxX3BnghOOVpWy8J1R9h9rJJ9RVUcOlGNS2FY73geuXYk4z1L3L298Sj/77VNTB6QxPPzx9PodPHWxqO89Nlh9hZVMWVgEt+bMYgpA5PO+m2/qt7B8u3HWLG7mKMnaygsr+N4RR0uhbAQYcawFOaMS2PG0BQiwqwZyZizeW/Hce74ay7/PXsE35yc6e9wOjVLBOeg3uHkg51F/GLJDgrK67h+bBpj+3Xn529vY0L/RP4yfwLREf++c1FVKa1uOOdv8w6ni71FVby18ShvbjhKSVU9ibER3DA+nZsnZrS5ycqYYNHgcHH5bz8mROCfd08j3PreWmWJ4DzUNDh46sN9PLfqAI1OZUJmIn+5dTyxkd7rZ3c4XazaW8Ir647w/s7jAHxleC++OTmTKQOTCLFFNozhL6sP8vA7O3h+fg6XDOvl73A6PUsEHWB/cRX/3HaMb03JJM6LSaC5o2W1vLzmMAs/z6O0uoF+STF8PSeduePSSOkW5bM4jOlMahucTH70A0b1TeCvt02wQRdtYIkgANQ1Onl3WyEL1+Wx9mApoSHCJcNSuHF8OtOH2L0QJri8u7WQ7768gZdvn8jUQR079DtQtZYI7M6LLiIqPJSvZafxtew0DhRX8VpuPovW5/PejuP07hbF3Jw0vp6TTnqi9SWYwLd0ayE94yKY2N+WoewIViPowhqdLj7YWcTCz4/w0R73ZHyXDkvhtqn9mdzKqCVjurLaBidjH3mP68f15RfXjvJ3OF2G1QgCVHhoCFeM7M0VI3s3DX/9+9oj3LRzLUN7xXPr1Ey+NrYvkWE2L7sJHCt2F1Hb6LSFZzqQNSwHiL7do/nRzKGsvu8S/r85owkJEe57cyszHlvJy2sP0+BwfflBjOkClm451SyU5O9QAobVCAJMVHho06iiT/aV8Jv39vDTt7bxhxX7WTBtAMnxkTQ6XTQ4XESEhXD5iN62kpPpMmoaHHy4q4g549IItWHUHcYSQYASES4anMyFg3ry8V53Qnhw8fYzyg3pFcfvbsxmeKot5GE6vxW7iqltdHLVaGsW6kiWCAKciDB9SDLTBvdkf3EVTheEhwrhoSHsPlbJ/W9tZfZTq/mvK4Zy29T+drOa6dSWbi0gOT6yacoX0zEsEQQJEWFQSvwXtqUnxpCd0Z0fv7GVXyzdyYrdRdw2tT9TB/W05iLT6ZxqFvp6Tro1C3UwSwRBLikukue+OY6/rzvCo8t28e0Xc4kKD+HCQclcOjyFCf0T6Z8UazUF43cf7iqirtHFVTZaqMNZIjCICDdP7MeccWmsPVDK+zuP88HOoqZ5jhKiw8nO6M64jB7MyUkjNSHazxGbYLR0SyEp8ZHkWLNQh7NEYJpEhoUybUgy04Yk8/A1yv7iKjYcLmPDkZNsPFLGE3v28OSHe7kuO43vTB/AgOS4pvfWNTo5Ud1An4Qou5HNdLiqenez0I3jrVnIGywRmBad6lMYlBLP18enA5BXWsNzqw7w6ud5vLY+j4uHJNPoVA6WVFNQXouqexTSDeMzuC67Lz1iI/z8W5hAsXRLAfUOF9dm9/V3KAHJppgw7VZcWc8Lnx7kH5sKSIqNILNnLJlJscRHhbFkSyGb8sqICA3h0uEppCfGEBsRRmxkKDGn/4wIJS4qjP49Y4mPslWlTOvmPP0pZbWNvPf/plmN8xzZ7KPGp3Ydq2Dhujz+ue0YZbUN1DW2fldzemI0w3t3Y0SfBGaN6s3gXvGtljfB5WBJNTN+vZL7rhzGndMH+jucLssSgfErh9NFTaOT6noHNQ1OauqdVDc4KK9tZO/xSnYWVrKzsIKDJ6pRhaz07nw9J52rx6TSzWoLQe+xf+3i6ZX7WXP/pbYGx3mwSeeMX4WFhtAtNKTFD/XLR/Ruel5SVc/bG4/y6ud5/OStrTy0eDuJsRHERYURHxVGj5gIbpvanwsH2/zzwcLpUt5Yf5TpQ5ItCXiRJQLTafSMi+T2iwbw7Qv7szm/nHe3FXKyuoGqegeVdQ52FVZwy5/XMjurDz+76gKS489tfWjTdazeV8Kxijoe+OoF/g4loFkiMJ2OiJCV3p2s9O5f2F7X6OQPK/fz9Mp9rNhVxI+vHMY1Y/pYZ3MAe319Pt1jwrl0eIq/QwlolghMlxEVHso9lw3hmjF9+OlbW/npW9v42dvbGJgcx5i07mRldOeiQT3J7Bnr71BNByivaeRf248xb3y6ranhZZYITJczKCWOhQsm8en+E6w/fJLNeWWs3F3EGxvyARiQHMulw1K4ZFgvJg1ItOGGXdQ7WwpocLiYMy7d36EEPEsEpksSEaYO6tm0cLmqcvhEDSt3F/HBriJe/PQwz606yKQBifzP10Z94S5o0zW8vj6fYb3jGdnXpkj3NluhzAQEESGzZyzzp/bnpW9PZOMDl/HItSPZXlDBFb9dxe/e30u9w+nvME0b7SioYHNeGXPGpVmNzgcsEZiAFBsZxjcm9eODe6Zz2Yhe/Ob9Pcz63Sr+tOoA+4ur6Gr3zwSbFz89RHR4KHOtWcgnvJYIROR5ESkSkW1n2X+xiJSLyCbP4wFvxWKCV0q3KP7vprH8Zf54wkJC+MXSnVz6+EdMf2wlD/5jGyt2F1HX2EJNQRXWrIG33nL/tMThMyerG3h701Guze5LQoyNCPMFb/YRvAA8Bfy1lTKrVPVqL8ZgDAAzhqUwY1gKeaXufoSVu4t5NTePFz87TGRYCJMHJjFjaArXje1L/IfvwXe+A2VlEBICLhd07w7PPAOzZvn7Vwl4Cz/Po97h4ltT+vk7lKDhtUSgqh+LSKa3jm/MuUhPjOEbkzP5xuRM6hqdrD1YyopdRXy0p5gHF29nwx9f5vHXfkFYfd0X31hVBXPmwKJFlgy8yOF08bc1h5k0IJFhva2T2Ff8PWposohsBgqA/1TVM1dXN8ZLosJDmT4kmelDkgHYeLiU9Kxbz0wCp9TWumsKR46AdWB6xfs7izhaVsvPr7Y7iX3Jn53FG4B+qjoG+D3w9tkKisgCEckVkdzi4mJfxWeCTHbhHpIcta0XKiuDdet8Ek8weuHTg/TtHs1X7E5in/JbIlDVClWt8jxfBoSLSIuzianqs6qao6o5ycnJPo3TBJHCQiTkS/4lQkKgoMA38QSZXccqWHOglFsm9SMs1AY0+pLf/toi0ls8A4RFZIInlhP+iscYUlPdHcOtcDicOHr1brWMOTcvfuruuL9xvA0Z9TWv9RGIyCvAxUBPEckHHgTCAVT1j8Ac4Lsi4gBqgRvVBncbf5o4ERIS3B3DZ1EUGs3sD6u49uQOrhubxvBU69DsCFvzy3lrYz7XZtkSp/5gC9MYc7ply9yjg2rP7CvQ6Gg2PP4cz8QOZcXuIhqdypi0BG6amMFXx/QhJsLfYy+6pm1Hy7npuTV0iw7n9Tsnk5oQ7e+QApKtUGZMeyxb9qX3EZRWN/CPTUd5Zd0R9hyvIj4yjK9m9SElPhKHU2l0uQgRYc64NAbaPEdnte1oOTf/aS1xkWEsXDCJ9MQYf4cUsCwRGNNequ7RQQUF0KcPTJjQ4pBRVSX38En+vvYIy7YWUu9wERoihIYITpcSKsL3ZgzizosHtGsq5fLaRv6wch+T+idx8dDkgJxvZ3uBOwnERlgS8AVLBMb4gMvl/l8KCXF/aBdX1vPIkh0s3lzAwORYHrl2JEmxkewvrmJ/URWFFXVcl92XnMzELxznaFktt/5lHXuOu/sqLkjtxn/MGMiVI1MJDen6CSGvtIaX1hzmlbVHiI8KY+GCyWQkWRLwNksExvjRyt1F/OztbeSf/GK/Q3R4KLWNTm6emMF/XTGMhOhwtheUc+tfPqe2wclTN4+lqKKOpz/az4Hiavr3jGVi/0QykmLISIwhMymW4andukxy+HRfCc+vPsgHu4oIEeHyEb24/8rhVhPwEUsExvhZTYODtzcWEBcVxsDkWPr3jEUVfvPeHp5ffZCecZF8c3I/nl65n27R4bxw6wSG9o4H3Au4/2v7MV767DB7iyopqWpoOm5KfCSzs/pwbXZfLkjthiocKKlm45GTbC+oQFWJigglOjyUmIhQRvXtzrh+PYgI893I8eLKeh56ZztLtxTSMy6CeRMyuGlihnUK+5glAmM6sa355dz35ha2F1QwrHc8L9w6gd4JUWctX1Xv4MiJGvYcr2Tp1kJWekYw9UuKobS6gco6BwAxEaGEh4ZQ2+ikwfHv+yNiIkKZPCCJCwf3ZGTfBAYmx5HohSGbqsobG47yyJId1DY4uevSQdwxrX19JabjWCIwppNzOF2s3F3MpIFJxEW2bxjqyeoGlm0r5IOdRfTqFkV2Rney07szMDmuqb/C6VIq6xpZd7CUVXtLWLW3mEMnapqO0SMmnEEpcUwfkszVo/uc17rPpdUNfLiriEXr81hzoJScfj149PrRDEqx0VP+ZInAGHOGo2W17Dleyf6iKvYXV7Oj0L0qGMDotAS+OroP4/snMqRXXIv3SNQ7nBRV1HOsoo7C8jrySmv4aE8xuYdKcSn07hbFf8wYyC0T+zUlJOM/lgiMMW1SUFbLki0FvLO5kK1HywH3qNl+iTEMSolv+vAvqqzjZE3jGe8f1juemRf04rILejOyb7eAHPbaVVkiMMa0W/7JGrYXVLCrsJLdxyvYe7yKmMgwUuIjPY8oUhOi6JXg+dktioRoW1Gss2otEdg98caYFqX1iCGtRwyXj7BJ9gKdzfVqjDFBzhKBMcYEOUsExhgT5CwRGGNMkLNEYIwxQc4SgTHGBDlLBMYYE+QsERhjTJDrcncWi0gxcLjZ5gSgvAMO397jtLV8a+XOZV9L25tv6wmUtCG2jtaVr0Vr+8/nWoB/roddi85zLc4Wi7ePcXr5fqqa3GIpVe3yD+BZfxynreVbK3cu+1ra3nwbkGvXov3l2vM3b+u18Nf1sGvRea5FR10Pb12LQGkaesdPx2lr+dbKncu+lrZ31N/gfHXla9HafrsWHV8+mK4FdEwsXrkWXa5pyLSNiOTqWSaYMr5n16PzsGtxpkCpEZgzPevvAMwX2PXoPOxaNGM1AmOMCXJWIzDGmCBnicAYY4KcJQJjjAlylgiMMSbIWSIIQiIyXET+KCKLROS7/o4nmInItSLynIi8KiIz/R1PMBORASLyZxFZ5O9YfM0SQRcjIs+LSJGIbGu2/QoR2S0i+0TkvtaOoao7VfVO4OvAVG/GG8g66Fq8rap3AHcCN3gz3kDWQdfigKp+27uRdk42fLSLEZFpQBXwV1Ud6dkWCuwBLgPygc+BeUAo8Ktmh7hNVYtE5Brgu8BLqvp3X8UfSDrqWnje9zjwsqpu8FH4AaWDr8UiVZ3jq9g7gzB/B2DaR1U/FpHMZpsnAPtU9QCAiCwEZqvqr4Crz3KcxcBiEVkKWCI4Bx1xLUREgEeBdy0JnLuO+r8IVtY0FBj6Anmnvc73bGuRiFwsIk+KyDPAMm8HF2TadS2AHwBfAeaIyJ3eDCwItff/IklE/ghki8j93g6uM7EaQRBS1ZXASj+HYQBVfRJ40t9xGFDVE7j7aoKO1QgCw1Eg/bTXaZ5txvfsWnQedi3ayBJBYPgcGCwi/UUkArgRWOznmIKVXYvOw65FG1ki6GJE5BXgM2CoiOSLyLdV1QF8H/gXsBN4TVW3+zPOYGDXovOwa3F+bPioMcYEOasRGGNMkLNEYIwxQc4SgTHGBDlLBMYYE+QsERhjTJCzRGCMMUHOEoEJGCJS5ePzferj83UXkf/w5TlNcLBEYMxZiEirc3Gp6hQfn7M7YInAdDhLBCagichAEfmniKwXkVUiMsyz/asislZENorI+yLSy7P9IRF5SURWAy95Xj8vIitF5ICI3HXasas8Py/27F8kIrtE5GXP9NKIyCzPtvWeGV+XtBDjfBFZLCIfAh+ISJyIfCAiG0Rkq4jM9hR9FBgoIptE5DHPe+8Vkc9FZIuIPOzNv6UJYKpqD3sExAOoamHbB8Bgz/OJwIee5z349531twOPe54/BKwHok97/SkQCfQETgDhp58PuBgoxz2pWQjuqQ4uBKJwT4Pc31PuFWBJCzHOxz1FcqLndRjQzfO8J7APECAT2Hba+2YCz3r2hQBLgGn+vg726HoPm4baBCwRiQOmAK97vqCD+wMd3B/ar4pIKhABHDztrYtVtfa010tVtR6oF5EioBfuD+7TrVPVfM95N+H+0K4CDqjqqWO/Aiw4S7jvqWrpqdCB//GsuuXCPYd+rxbeM9Pz2Oh5HQcMBj4+yzmMaZElAhPIQoAyVc1qYd/vgSdUdbGIXIz7m/8p1c3K1p/23EnL/zdtKdOa0895M5AMjFPVRhE5hLt20ZwAv1LVZ9p5LmO+wPoITMBS1QrgoIjMBfeykCIyxrM7gX/PTf8tL4WwGxhw2hKKbV2cPgEo8iSBGUA/z/ZKIP60cv8CbvPUfBCRviKScv5hm2BjNQITSGJE5PQmmydwf7t+WkR+BoQDC4HNuGsAr4vISeBDoH9HB6OqtZ7hnv8UkWrc8+O3xcvAOyKyFcgFdnmOd0JEVovINtxrHN8rIsOBzzxNX1XALUBRR/8uJrDZNNTGeJGIxKlqlWcU0f8Be1X1N/6Oy5jTWdOQMd51h6fzeDvuJh9rzzedjtUIjDEmyFmNwBhjgpwlAmOMCXKWCIwxJshZIjDGmCBnicAYY4Lc/w8KD/sogRwSVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | model         | MultiImage       | 33.1 M\n",
      "1 | loss_function | MSELoss          | 0     \n",
      "2 | train_mse     | MeanSquaredError | 0     \n",
      "3 | valid_mse     | MeanSquaredError | 0     \n",
      "---------------------------------------------------\n",
      "21.9 M    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "33.1 M    Total params\n",
      "132.205   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006579332246575679\n",
      "EPOCH NUMERO: 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2c1364aa6846b18e7b524f4aa54da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Epoch_List = [20]\n",
    "num = len(os.listdir(\"../../../tb_logs/my_model_diff\"))\n",
    "for eps in Epoch_List:\n",
    "    \n",
    "    # 1 - Se genera la red\n",
    "    \n",
    "    # To pretrain\n",
    "    model = models.resnet18(pretrained=False)\n",
    "\n",
    "    # Change the first convolution\n",
    "    new_conv = torch.nn.Conv2d(18, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model = list(model.children())[1:]\n",
    "    model = [new_conv] + model\n",
    "    model = torch.nn.Sequential(*model)\n",
    "\n",
    "    # Get only the feature extractor (remove avgpool and fc layers)\n",
    "    fe = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "    fe.apply(freeze_all_but_bn) # Solo usar si no se preentrena la red\n",
    "\n",
    "    # Create custom head\n",
    "    head_clf = nn.Sequential(\n",
    "        nn.BatchNorm1d(512+512+4),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512+512+4, 512), \n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm1d(512),   \n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512, 4)\n",
    "    )\n",
    "    \n",
    "    num_layers = 10\n",
    "    dropout = 0.2\n",
    "    bidirectional = False\n",
    "    hidden = (torch.randn(num_layers, BATCH_SIZE, 512+4).cuda(), torch.randn(num_layers, BATCH_SIZE, 512+4).cuda())\n",
    "    lstm = nn.LSTM(np.shape(hidden[0])[2], np.shape(hidden[0])[2], num_layers = num_layers, bidirectional = bidirectional, dropout = dropout)\n",
    "\n",
    "    # Rebuild the model\n",
    "    model = MultiImage(fe, head_clf, lstm, num_layers, hidden)\n",
    "\n",
    "    # 2 - Se busca el mejor Learning Rate a aplicar como maximo posible\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # We catch the optimizer of our lit_model\n",
    "    # optimizer, lr_scheduler = pl_model.configure_optimizers()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Learning State Finder\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "    lr_finder.range_test(train_loader, end_lr=1, num_iter=100, step_mode=\"exp\")\n",
    "\n",
    "    # Create trainer class\n",
    "    try:\n",
    "        BEST_LR = lr_finder.plot()[1]\n",
    "    except:\n",
    "        BEST_LR = 5.0E-03\n",
    "    print(BEST_LR)\n",
    "    \n",
    "    if BEST_LR > 0.09:\n",
    "        BEST_LR = 5.0E-03\n",
    "\n",
    "    # 3 - Se prepara el Trainer y se entrena con el modelo LitModel\n",
    "    \n",
    "    # Number of iterations for LR Schedule\n",
    "    ITERATIONS_PER_EPOCH = len(train_loader)\n",
    "    \n",
    "    logger = TensorBoardLogger(\"/home/hodei.zia/ImitAI Project/tb_logs\", name=\"my_model_diff\")\n",
    "\n",
    "    # Create PytorchLighting model for training\n",
    "    pl_model = LitModel(model, criterion, total_iterations=ITERATIONS_PER_EPOCH * eps, lr=BEST_LR)\n",
    "\n",
    "    NUM_GPUS = 1 if torch.cuda.is_available() else 0\n",
    "        \n",
    "    numCP = len(os.listdir(\"/home/hodei.zia/ImitAI Project/Pruebas ImitAI/Dataset_Mujoco/CP2/\"))\n",
    "    \n",
    "    # Create de ModelCheckpoint\n",
    "    checkpoint_callback  = pl.callbacks.ModelCheckpoint(dirpath='CP2/CP'+str(numCP+1), \n",
    "                                                        save_top_k=-1, \n",
    "                                                        auto_insert_metric_name=True, \n",
    "                                                        every_n_epochs = 1)\n",
    "    \n",
    "    # Create trainer class\n",
    "    trainer = pl.Trainer(auto_lr_find=True, min_epochs=eps, max_epochs=eps,\n",
    "                         log_every_n_steps=1, gpus=NUM_GPUS, callbacks=[CallbackClass(what=\"epochs\"), checkpoint_callback], logger=logger)\n",
    "    #trainer = pl.Trainer(auto_lr_find=True, min_epochs=eps, max_epochs=eps,\n",
    "    #                     log_every_n_steps=1, gpus=NUM_GPUS, callbacks=[checkpoint_callback], logger=logger)\n",
    "\n",
    "    print(\"EPOCH NUMERO: \" + str(eps))\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(pl_model, train_loader, val_dataloaders=test_loader)\n",
    "    \n",
    "    #num = len(os.listdir(\"/home/hodei.zia/ImitAI Project/tb_logs/my_model\"))\n",
    "    erroresTrainEpoch=[]\n",
    "    erroresValEpoch=[]\n",
    "    erroresTrainStep=[]\n",
    "    erroresValStep=[]\n",
    "    \n",
    "    PATH_TB = os.listdir(\"../../../tb_logs/my_model_diff/version_\" + str(num))[1]\n",
    "    PATH = \"/home/hodei.zia/ImitAI Project/tb_logs/my_model_diff/version_\" + str(num) + \"/\"\n",
    "    path_to_events_file = PATH + PATH_TB\n",
    "    num = num + 1\n",
    "    \n",
    "    # Plot de LOSS \n",
    "    for e in tf.compat.v1.train.summary_iterator(path_to_events_file):\n",
    "        for v in e.summary.value:\n",
    "            if v.tag == 'training_loss_epoch':\n",
    "                erroresTrainEpoch.append(v.simple_value)\n",
    "            if v.tag == 'valid_loss_epoch':\n",
    "                erroresValEpoch.append(v.simple_value)\n",
    "\n",
    "            if v.tag == 'training_loss_step':\n",
    "                erroresTrainStep.append(v.simple_value)\n",
    "            #if v.tag == 'valid_loss_step':\n",
    "            #    erroresValStep.append(v.simple_value)\n",
    "\n",
    "    plt.plot(erroresTrainEpoch, label='Train')\n",
    "    plt.plot(erroresValEpoch, label='Validacion')\n",
    "    plt.title(\"Loss de train y validacion por epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(erroresTrainStep, label='Train')\n",
    "    #plt.plot(erroresValStep, label='Validacion')\n",
    "    plt.title(\"Loss de train por steps\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4 - Se predice el resultado\n",
    "    \n",
    "    # Put model on eval mode so we change the BatchNorm and Dropout layers behaviour\n",
    "    model = model.eval()\n",
    "    # Set device (\"cuda\" or GPU if cuda is installed, otherwise in \"cpu\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # To concatenate predictions and labels\n",
    "    predictions = None\n",
    "    labels = None\n",
    "    # Surround all for not computing gradients\n",
    "    with torch.no_grad():\n",
    "        # Iterate over test dataset\n",
    "        for x, y in tqdm.notebook.tqdm(test_loader):\n",
    "            # Move to device\n",
    "            x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11 = x\n",
    "            x1 = x1.to(device)\n",
    "            x2 = x2.to(device)\n",
    "            x3 = x3.to(device)\n",
    "            x4 = x4.to(device)\n",
    "            x5 = x5.to(device)\n",
    "            x6 = x6.to(device)\n",
    "            x7 = x7.to(device)\n",
    "            x8 = x8.to(device)\n",
    "            x9 = x9.to(device)\n",
    "            x10 = x10.to(device)\n",
    "            x11 = x11.to(device)\n",
    "            \n",
    "            # Predict\n",
    "            o = model((x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11))\n",
    "            # Concatenate\n",
    "            if predictions is None:\n",
    "                predictions = o\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, o), axis=0)\n",
    "\n",
    "            if labels is None:\n",
    "                labels = y\n",
    "            else:\n",
    "                labels = torch.cat((labels, y), axis=0)\n",
    "\n",
    "        # Move predictions to CPU and to numpy\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    mse = sklearn.metrics.mean_squared_error(labels, predictions)\n",
    "    rmse = sqrt(mse)\n",
    "    \n",
    "    print(\"El valor del error cudratico medios (MSE) es igual a \" + str(mse))\n",
    "    print(\"El valor RMSE es igual a \" + str(rmse))\n",
    "\n",
    "    # 5 - Se guardan los resultados \n",
    "    \n",
    "    RMSE_List.append(rmse)\n",
    "    Model_List.append(model)\n",
    "    Prediction_List.append(predictions)\n",
    "    Label_List.append(labels)\n",
    "    LR_List.append(BEST_LR)\n",
    "    #Loss_List.append(lr_finder.history['loss'][lr_finder.history['lr'].index(BEST_LR)])\n",
    "    \n",
    "    print(RMSE_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(os.listdir(\"/home/hodei.zia/ImitAI Project/tb_logs/my_model\"))\n",
    "erroresTrainEpoch=[]\n",
    "erroresValEpoch=[]\n",
    "erroresTrainStep=[]\n",
    "erroresValStep=[]\n",
    "\n",
    "PATH_TB = os.listdir(\"/home/hodei.zia/ImitAI Project/tb_logs/my_model/version_\" + str(num-1))[0]\n",
    "PATH = \"/home/hodei.zia/ImitAI Project/tb_logs/my_model/version_\" + str(num-1) + \"/\"\n",
    "path_to_events_file = PATH + PATH_TB\n",
    "\n",
    "# Plot de LOSS \n",
    "for e in tf.compat.v1.train.summary_iterator(path_to_events_file):\n",
    "    for v in e.summary.value:\n",
    "        if v.tag == 'training_loss_epoch':\n",
    "            erroresTrainEpoch.append(v.simple_value)\n",
    "        if v.tag == 'valid_loss_epoch':\n",
    "            erroresValEpoch.append(v.simple_value)\n",
    "\n",
    "        if v.tag == 'training_loss_step':\n",
    "            erroresTrainStep.append(v.simple_value)\n",
    "        #if v.tag == 'valid_loss_step':\n",
    "        #    erroresValStep.append(v.simple_value)\n",
    "\n",
    "plt.plot(erroresTrainEpoch, label='Train')\n",
    "plt.plot(erroresValEpoch, label='Validacion')\n",
    "plt.title(\"Loss de train y validacion por epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylim(0,0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(erroresTrainStep, label='Train')\n",
    "#plt.plot(erroresValStep, label='Validacion')\n",
    "plt.title(\"Loss de train por steps\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylim(0,0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480d9ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(\"BEST RMSE: \" + str(min(RMSE_List)) + \" in Epoch \" + \n",
    "      str(Epoch_List[(RMSE_List.index(min(RMSE_List)))]) + \" with Batch Size = \" + str(BATCH_SIZE) + \" \\nLR = \" + \n",
    "      str(LR_List[(RMSE_List.index(min(RMSE_List)))]))\n",
    "\n",
    "# Plot de RMSE-Epoch\n",
    "plt.plot(Epoch_List, RMSE_List, color='blue', linewidth = 3, marker='o', markerfacecolor='red', markersize=12)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Evolucion del RMSE por Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Plot del LR-Epoch\n",
    "plt.plot(Epoch_List, LR_List, color='blue', linewidth = 3, marker='o', markerfacecolor='red', markersize=12)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('LR')\n",
    "plt.title('Evolucion del LR por Epochs')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df1e39",
   "metadata": {},
   "source": [
    "## Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96a94a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Num Epochs mejor prueba: \"+str(Epoch_List[RMSE_List.index(min(RMSE_List))]))\n",
    "print(\"RMSE mejor prueba: \"+str(RMSE_List[RMSE_List.index(min(RMSE_List))]))\n",
    "print(\"\")\n",
    "\n",
    "labels = Label_List[(RMSE_List.index(min(RMSE_List)))]\n",
    "predictions = Prediction_List[(RMSE_List.index(min(RMSE_List)))]\n",
    "\n",
    "# Error promedio de cada una de las articulaciones\n",
    "joint_error = np.zeros(3)\n",
    "minimo = np.zeros(3)\n",
    "maximo = np.zeros(3)\n",
    "for i in range(3):\n",
    "    joint_error[i] = sqrt(sklearn.metrics.mean_squared_error(predictions[:, i], labels[:, i]))\n",
    "    minimo[i] = min(labels[:, i])\n",
    "    maximo[i] = max(labels[:, i])\n",
    "    \n",
    "print(\"Error total por articulacion: \"+str(joint_error))\n",
    "\n",
    "ejes = [\"X\", \"Y\", \"Z\"]\n",
    "for i in range(3):\n",
    "    print(\"Eje \" + ejes[i] + \" [\" + str(round(minimo[i], 2)) + \" - \" + str(round(maximo[i], 2)) + \"] \" +\n",
    "          str(round(abs(round(minimo[i], 2) - round(maximo[i], 2)),2))\n",
    "          , str(round(joint_error[i]*100/round(abs(round(minimo[i], 2) - round(maximo[i], 2)),2),2)) + \"%\")\n",
    "    \n",
    "#MSE por demostracion\n",
    "mseDem=[]\n",
    "rmseDem=[]\n",
    "for j in range(len(pd.read_csv(\"Test_dataset/Test_Actions.csv\", index_col=0))//100-1):\n",
    "    mseDem.append(sklearn.metrics.mean_squared_error(labels[j*100:(j+1)*100], predictions[j*100:(j+1)*100]))\n",
    "    rmseDem.append(sqrt(mseDem[j]))\n",
    "    \n",
    "print(\"Los valores RMSE por demostracion son igual a \"+str(rmseDem[0:5]))\n",
    "print(\"El valor medio de RMSE es \"+str(np.mean(rmseDem)))\n",
    "print(\"El valor maximo de RMSE es \"+str(max(rmseDem))+\" y equivale a la prueba \"+str(np.argmax(rmseDem)))\n",
    "print(\"El valor minimo de RMSE es \"+str(min(rmseDem))+\" y equivale a la prueba \"+str(np.argmin(rmseDem)))\n",
    "print(\"\")\n",
    "mRMSE=sorted(rmseDem)[0:5]\n",
    "print(\"Mejores RMSE: \"+str(mRMSE))\n",
    "print(\"Mejores pruebas: \"+str(rmseDem.index(mRMSE[0]))+\", \"+str(rmseDem.index(mRMSE[1]))+\", \"+str(rmseDem.index(mRMSE[2]))+\", \"+str(rmseDem.index(mRMSE[3]))+\", \"+str(rmseDem.index(mRMSE[4])))\n",
    "print(\"Comienzan en la imagen: \"+str(rmseDem.index(mRMSE[0])*100)+\", \"+str(rmseDem.index(mRMSE[1])*100)+\", \"+str(rmseDem.index(mRMSE[2])*100)+\", \"+str(rmseDem.index(mRMSE[3])*100)+\", \"+str(rmseDem.index(mRMSE[4])*100))\n",
    "print(\"\")\n",
    "pRMSE=sorted(rmseDem,reverse=True)[0:5]\n",
    "print(\"Peores RMSE: \"+str(pRMSE))\n",
    "print(\"Peores pruebas: \"+str(rmseDem.index(pRMSE[0]))+\", \"+str(rmseDem.index(pRMSE[1]))+\", \"+str(rmseDem.index(pRMSE[2]))+\", \"+str(rmseDem.index(pRMSE[3]))+\", \"+str(rmseDem.index(pRMSE[4])))\n",
    "print(\"Comienzan en la imagen: \"+str(rmseDem.index(pRMSE[0])*100)+\", \"+str(rmseDem.index(pRMSE[1])*100)+\", \"+str(rmseDem.index(pRMSE[2])*100)+\", \"+str(rmseDem.index(pRMSE[3])*100)+\", \"+str(rmseDem.index(pRMSE[4])*100))\n",
    "print(\"\")\n",
    "\n",
    "#Ploteamos la primera prueba\n",
    "j=1\n",
    "print(\"RMSE:\",sqrt(sklearn.metrics.mean_squared_error(labels[j*100:(j+1)*100], predictions[j*100:(j+1)*100])))\n",
    "for i in range(3):\n",
    "    plt.plot(predictions[j*100:(j+1)*100, i], label='prediction')\n",
    "    plt.plot(labels[j*100:(j+1)*100, i], label='target')\n",
    "    plt.title(\"Observacion: \" + str(j) +\" - Articulacion \" + str(i+1))\n",
    "    plt.ylim(-3,3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#Ploteamos la peor prueba\n",
    "j=np.argmax(rmseDem)\n",
    "print(\"RMSE:\",sqrt(sklearn.metrics.mean_squared_error(labels[j*100:(j+1)*100], predictions[j*100:(j+1)*100])))\n",
    "for i in range(3):\n",
    "    plt.plot(predictions[j*100:(j+1)*100, i], label='prediction')\n",
    "    plt.plot(labels[j*100:(j+1)*100, i], label='target')\n",
    "    plt.title(\"Observacion: \" + str(j) +\" - Articulacion \" + str(i+1))\n",
    "    plt.ylim(-3,3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#Ploteamos la mejor prueba\n",
    "j=np.argmin(rmseDem)\n",
    "print(\"RMSE:\",sqrt(sklearn.metrics.mean_squared_error(labels[j*100:(j+1)*100], predictions[j*100:(j+1)*100])))\n",
    "for i in range(3):\n",
    "    plt.plot(predictions[j*100:(j+1)*100, i], label='prediction')\n",
    "    plt.plot(labels[j*100:(j+1)*100, i], label='target')\n",
    "    plt.title(\"Observacion: \" + str(j) +\" - Articulacion \" + str(i+1))\n",
    "    plt.ylim(-3,3)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc9740",
   "metadata": {},
   "source": [
    "## Histrogramas de RMSE por demostraciones Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee131a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rmseDem,np.shape(rmseDem)[0],label=\"RMSE\")\n",
    "#plt.bar(np.arange(np.shape(rmseDem)[0]),rmseDem,label=\"RMSE\")\n",
    "#plt.plot(np.arange(np.shape(rmseDem)[0]),rmseDem,label=\"RMSE\")\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Demostracion')\n",
    "plt.title('Histrogrma de rangos de RMSE test')\n",
    "plt.xlim(0,0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.hist(rmseDem,np.shape(rmseDem)[0],label=\"RMSE\")\n",
    "plt.bar(np.arange(np.shape(rmseDem)[0]),rmseDem,label=\"RMSE\")\n",
    "#plt.plot(np.arange(np.shape(rmseDem)[0]),rmseDem,label=\"RMSE\")\n",
    "plt.xlabel('Demostracion')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Histrogrma de RMSE por demostraciones test')\n",
    "plt.ylim(0,0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fd9b1",
   "metadata": {},
   "source": [
    "## Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55503a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fecha del dia en el que se guarda el modelo\n",
    "#Nombre del modelo (en formato acordaado en abril)\n",
    "#Imagenes utilizadas\n",
    "#Acciones utilizadas\n",
    "#Batch_size\n",
    "#Numero de epochs\n",
    "#Prediccion del modelo\n",
    "#Explicaciones extras\n",
    "#RMSE\n",
    "#Errores de cada Eje\n",
    "#Accuracy en el simulador\n",
    "nueva_fila = {'Fecha': datetime.today().strftime('%Y-%m-%d'), \n",
    "              'Modelo': \"4-0123456789_32_25_Pant_ImYAcT-5-LSTMImgYAc_P_0.5104\", \n",
    "              'Imagenes': \"Hasta T-5\", \n",
    "              'Acciones': \"T-5\", \n",
    "              'Batch_size': BATCH_SIZE, \n",
    "              'Epochs': Epoch_List[0], \n",
    "              'Prediccion': \"Posicion\",\n",
    "              'Comentario': \"Diferencia T-T_1\",\n",
    "              'RMSE':str(round(RMSE_List[RMSE_List.index(min(RMSE_List))], 6)), \n",
    "              'EjeX':str(round(joint_error[0]*100/round(abs(round(minimo[0], 2) - round(maximo[0], 2)),2),2)) + \" %\", \n",
    "              'EjeY':str(round(joint_error[1]*100/round(abs(round(minimo[1], 2) - round(maximo[1], 2)),2),2)) + \" %\", \n",
    "              'EjeZ':str(round(joint_error[2]*100/round(abs(round(minimo[2], 2) - round(maximo[2], 2)),2),2)) + \" %\", \n",
    "              'AccSim':str(round(0/50*100,2)) + \" %\"}\n",
    "\n",
    "print(nueva_fila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69720af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"resultados.csv\", index_col=0)\n",
    "\n",
    "df = df.append(nueva_fila, ignore_index=True)\n",
    " \n",
    "print(df)\n",
    "\n",
    "df.to_csv('resultados.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ee513",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATOS DEL MODELO:\")\n",
    "print(\" - Numero de Epoch: \" + str(Epoch_List[(RMSE_List.index(min(RMSE_List)))]))\n",
    "print(\" - RMSE: \" + str(min(RMSE_List)))\n",
    "torch.save(Model_List[(RMSE_List.index(min(RMSE_List)))], 'model_pytorch_05104')\n",
    "model = torch.load('model_pytorch_05104')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f231fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImitAI",
   "language": "python",
   "name": "imitai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
